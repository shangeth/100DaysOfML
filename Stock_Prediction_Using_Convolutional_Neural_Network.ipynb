{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Prediction Using Convolutional Neural Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SyougUQUEWGa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R5nNBYkPE7CE",
        "colab_type": "code",
        "outputId": "0ba0eeb7-98f6-4e0b-caaf-5026f54b2557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GQHV0guAioUb",
        "colab_type": "code",
        "outputId": "5080dc5b-6d77-43f7-f269-b8fea7ead39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "df = pd.read_csv('SUNTV.csv')\n",
        "df = df.drop(['Date Time'], 1)\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "\n",
        "def classification_data(df, interval=10):\n",
        "  c=[]\n",
        "  for i, row in df.iterrows():\n",
        "    try:\n",
        "      if df.iloc[i]['Close'] < df.iloc[i+interval]['Close']:\n",
        "        c.append(1)\n",
        "      else:\n",
        "        c.append(0)\n",
        "    except: \n",
        "      try:\n",
        "        df =df.drop(df.index[[i]])\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "  df = df.iloc[:len(c)]\n",
        "  se = pd.Series(c)\n",
        "  df['class'] = se.values\n",
        "  df.tail()\n",
        "  return df\n",
        "\n",
        "df = classification_data(df)\n",
        "\n",
        "###################################################################\n",
        "\n",
        "\n",
        "y = df['class'].values\n",
        "X = df.drop('class', 1).values\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(X)\n",
        "\n",
        "########################################################################\n",
        "import numpy as np\n",
        "\n",
        "X_n = []\n",
        "y_n = []\n",
        "\n",
        "for i in range(len(x_scaled) - 1000):\n",
        "    X_n.append(x_scaled[i:i+1000].T)\n",
        "    y_n.append(np.array(y[i:i+1000]))\n",
        "    \n",
        "##########################################################################3\n",
        "X_train = X_n[:int(len(X_n)*0.8)]\n",
        "X_test = X_n[len(X_n[:int(len(X_n)*0.8)]):]\n",
        "\n",
        "y_train = y_n[:int(len(y_n)*0.8)]\n",
        "y_test = y_n[len(y_n[:int(len(y_n)*0.8)]):]\n",
        "\n",
        "import torch.utils.data as utils\n",
        "tensor_x_tr = torch.stack([torch.Tensor(i) for i in X_n]) \n",
        "tensor_y_tr = torch.stack([torch.Tensor(i) for i in y_n])\n",
        "my_dataset_tr = utils.TensorDataset(tensor_x_tr,tensor_y_tr) \n",
        "trainloader = utils.DataLoader(my_dataset_tr, batch_size=1024) \n",
        "\n",
        "###\n",
        "\n",
        "tensor_x_te = torch.stack([torch.Tensor(i) for i in X_test]) \n",
        "tensor_y_te = torch.stack([torch.Tensor(i) for i in y_test])\n",
        "my_dataset_te = utils.TensorDataset(tensor_x_te,tensor_y_te) \n",
        "testloader = utils.DataLoader(my_dataset_te, batch_size=1024) \n",
        "\n",
        "##############################################################################\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvModel(nn.Module):\n",
        "  def __init__(self, input_channel=5, kernel_size=10):\n",
        "    super(ConvModel, self).__init__()\n",
        "    self.c1 = nn.Conv1d(input_channel, 32, kernel_size)\n",
        "    self.c2 = nn.Conv1d(32, 64, kernel_size)\n",
        "    self.c3 = nn.Conv1d(64, 128, kernel_size)\n",
        "    self.c4 = nn.Conv1d(128, 256, kernel_size)\n",
        "    self.c5 = nn.Conv1d(256, 256, kernel_size)\n",
        "    self.c6 = nn.Conv1d(256, 256, kernel_size)\n",
        "    self.mp = nn.MaxPool1d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.lrelu = nn.LeakyReLU(0.01)\n",
        "    self.dp = nn.Dropout(0.8)\n",
        "    self.fc1 = nn.Linear(3328, 2000)\n",
        "    self.fc2 = nn.Linear(2000, 1000)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.c1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.mp(x)\n",
        "    x = self.c2(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = self.mp(x)\n",
        "    \n",
        "    \n",
        "    x = self.c3(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = self.mp(x)\n",
        "    x = self.c4(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = self.mp(x)\n",
        "    x = self.c5(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = self.mp(x)\n",
        "    x = self.c6(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.dp(x)\n",
        "    x = self.fc2(x)\n",
        "#     x = self.sigmoid(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    return x\n",
        "    \n",
        "    \n",
        "    ########################################################################33\n",
        "    \n",
        "model =  ConvModel(5)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "##################################################################################33\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "epochs = 15\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 1\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in trainloader:\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "        labels = labels.long()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        print(f'labels s={labels.view(-1).shape}, data s={inputs.shape}')\n",
        "        logps = model(inputs)\n",
        "        print(f'logps shape{logps.view(-1).shape}')\n",
        "        loss = criterion(logps.view(1,-1), labels.view(1,-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "#         if steps % print_every == 0:\n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "    #                     inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "                labels=labels.long()\n",
        "#                 print(f'labels shape = {labels.shape} ')\n",
        "                print(f'labels s={labels.shape}, data s={inputs.shape}')\n",
        "                logps = model.forward(inputs)\n",
        "                batch_loss = criterion(logps, labels)\n",
        "\n",
        "                test_loss += batch_loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                ps = torch.exp(logps)\n",
        "                print(f'epoch={epoch} ps shape = {ps.shape}')\n",
        "                print(f'labels shape = {labels.shape} ')\n",
        "#                 top_p, top_class = ps.topk(1, dim=1)\n",
        "#                 print('topp shape= ',top_p.shape)\n",
        "                equals = ps == labels\n",
        "#   .view(*ps.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "    #                     print('acc=',accuracy)\n",
        "\n",
        "#         print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "#               f\"Train loss: {running_loss/print_every}.. \"\n",
        "#               f\"Test loss: {test_loss/len(testloader)}.. \"\n",
        "#               f\"Test accuracy: {accuracy/len(testloader)}\")\n",
        "        running_loss = 0\n",
        "        model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels s=torch.Size([1024000]), data s=torch.Size([1024, 5, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Plgm4NgujeKC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puFhlnDYjeM1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxZdWax6jeR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fdTDjOj8jeU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fgckPeTmjeQF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1D5hsrcEQ-a",
        "colab_type": "code",
        "outputId": "d2fa6047-ca27-42f3-8798-6a129815452d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('SUNTV.csv')\n",
        "df = df.drop(['Date Time'], 1)\n",
        "df.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>411.15</td>\n",
              "      <td>413.00</td>\n",
              "      <td>404.10</td>\n",
              "      <td>409.00</td>\n",
              "      <td>176240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>414.05</td>\n",
              "      <td>414.70</td>\n",
              "      <td>410.00</td>\n",
              "      <td>411.15</td>\n",
              "      <td>75844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>410.20</td>\n",
              "      <td>414.40</td>\n",
              "      <td>410.00</td>\n",
              "      <td>414.40</td>\n",
              "      <td>40789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>410.25</td>\n",
              "      <td>412.50</td>\n",
              "      <td>410.00</td>\n",
              "      <td>410.20</td>\n",
              "      <td>17987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>410.00</td>\n",
              "      <td>410.35</td>\n",
              "      <td>408.80</td>\n",
              "      <td>410.35</td>\n",
              "      <td>57179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>410.25</td>\n",
              "      <td>411.25</td>\n",
              "      <td>408.50</td>\n",
              "      <td>410.00</td>\n",
              "      <td>81484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>409.00</td>\n",
              "      <td>410.25</td>\n",
              "      <td>409.00</td>\n",
              "      <td>410.00</td>\n",
              "      <td>46254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>415.75</td>\n",
              "      <td>419.40</td>\n",
              "      <td>410.00</td>\n",
              "      <td>410.00</td>\n",
              "      <td>171503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>415.75</td>\n",
              "      <td>416.25</td>\n",
              "      <td>413.20</td>\n",
              "      <td>415.75</td>\n",
              "      <td>116767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>414.15</td>\n",
              "      <td>416.90</td>\n",
              "      <td>411.75</td>\n",
              "      <td>415.75</td>\n",
              "      <td>78892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>413.90</td>\n",
              "      <td>416.75</td>\n",
              "      <td>413.70</td>\n",
              "      <td>413.80</td>\n",
              "      <td>37042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>414.60</td>\n",
              "      <td>415.45</td>\n",
              "      <td>413.00</td>\n",
              "      <td>413.90</td>\n",
              "      <td>21333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>414.50</td>\n",
              "      <td>415.90</td>\n",
              "      <td>413.60</td>\n",
              "      <td>414.95</td>\n",
              "      <td>64999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>415.15</td>\n",
              "      <td>415.90</td>\n",
              "      <td>414.15</td>\n",
              "      <td>414.85</td>\n",
              "      <td>28525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>414.10</td>\n",
              "      <td>418.80</td>\n",
              "      <td>413.55</td>\n",
              "      <td>415.15</td>\n",
              "      <td>152511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>414.25</td>\n",
              "      <td>416.55</td>\n",
              "      <td>414.25</td>\n",
              "      <td>414.70</td>\n",
              "      <td>75048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>414.25</td>\n",
              "      <td>415.05</td>\n",
              "      <td>413.30</td>\n",
              "      <td>414.25</td>\n",
              "      <td>15949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>415.00</td>\n",
              "      <td>415.00</td>\n",
              "      <td>410.80</td>\n",
              "      <td>414.00</td>\n",
              "      <td>95718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>413.55</td>\n",
              "      <td>416.70</td>\n",
              "      <td>412.60</td>\n",
              "      <td>414.50</td>\n",
              "      <td>46124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>413.95</td>\n",
              "      <td>414.80</td>\n",
              "      <td>412.20</td>\n",
              "      <td>413.50</td>\n",
              "      <td>39154</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Close    High     Low    Open  Volume\n",
              "0   411.15  413.00  404.10  409.00  176240\n",
              "1   414.05  414.70  410.00  411.15   75844\n",
              "2   410.20  414.40  410.00  414.40   40789\n",
              "3   410.25  412.50  410.00  410.20   17987\n",
              "4   410.00  410.35  408.80  410.35   57179\n",
              "5   410.25  411.25  408.50  410.00   81484\n",
              "6   409.00  410.25  409.00  410.00   46254\n",
              "7   415.75  419.40  410.00  410.00  171503\n",
              "8   415.75  416.25  413.20  415.75  116767\n",
              "9   414.15  416.90  411.75  415.75   78892\n",
              "10  413.90  416.75  413.70  413.80   37042\n",
              "11  414.60  415.45  413.00  413.90   21333\n",
              "12  414.50  415.90  413.60  414.95   64999\n",
              "13  415.15  415.90  414.15  414.85   28525\n",
              "14  414.10  418.80  413.55  415.15  152511\n",
              "15  414.25  416.55  414.25  414.70   75048\n",
              "16  414.25  415.05  413.30  414.25   15949\n",
              "17  415.00  415.00  410.80  414.00   95718\n",
              "18  413.55  416.70  412.60  414.50   46124\n",
              "19  413.95  414.80  412.20  413.50   39154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "TEdcX5hXEQ-m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def classification_data(df, interval=10):\n",
        "  c=[]\n",
        "  for i, row in df.iterrows():\n",
        "    try:\n",
        "      if df.iloc[i]['Close'] < df.iloc[i+interval]['Close']:\n",
        "        c.append(1)\n",
        "      else:\n",
        "        c.append(0)\n",
        "    except: \n",
        "      try:\n",
        "        df =df.drop(df.index[[i]])\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "  df = df.iloc[:len(c)]\n",
        "  se = pd.Series(c)\n",
        "  df['class'] = se.values\n",
        "  df.tail()\n",
        "  return df\n",
        "\n",
        "df = classification_data(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gLgJlzU8EQ-q",
        "colab_type": "code",
        "outputId": "7f32e0f6-c1dc-4d22-a631-85ccd398b998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>411.15</td>\n",
              "      <td>413.00</td>\n",
              "      <td>404.10</td>\n",
              "      <td>409.00</td>\n",
              "      <td>176240</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>414.05</td>\n",
              "      <td>414.70</td>\n",
              "      <td>410.00</td>\n",
              "      <td>411.15</td>\n",
              "      <td>75844</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>410.20</td>\n",
              "      <td>414.40</td>\n",
              "      <td>410.00</td>\n",
              "      <td>414.40</td>\n",
              "      <td>40789</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>410.25</td>\n",
              "      <td>412.50</td>\n",
              "      <td>410.00</td>\n",
              "      <td>410.20</td>\n",
              "      <td>17987</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>410.00</td>\n",
              "      <td>410.35</td>\n",
              "      <td>408.80</td>\n",
              "      <td>410.35</td>\n",
              "      <td>57179</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>410.25</td>\n",
              "      <td>411.25</td>\n",
              "      <td>408.50</td>\n",
              "      <td>410.00</td>\n",
              "      <td>81484</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>409.00</td>\n",
              "      <td>410.25</td>\n",
              "      <td>409.00</td>\n",
              "      <td>410.00</td>\n",
              "      <td>46254</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>415.75</td>\n",
              "      <td>419.40</td>\n",
              "      <td>410.00</td>\n",
              "      <td>410.00</td>\n",
              "      <td>171503</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>415.75</td>\n",
              "      <td>416.25</td>\n",
              "      <td>413.20</td>\n",
              "      <td>415.75</td>\n",
              "      <td>116767</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>414.15</td>\n",
              "      <td>416.90</td>\n",
              "      <td>411.75</td>\n",
              "      <td>415.75</td>\n",
              "      <td>78892</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>413.90</td>\n",
              "      <td>416.75</td>\n",
              "      <td>413.70</td>\n",
              "      <td>413.80</td>\n",
              "      <td>37042</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>414.60</td>\n",
              "      <td>415.45</td>\n",
              "      <td>413.00</td>\n",
              "      <td>413.90</td>\n",
              "      <td>21333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>414.50</td>\n",
              "      <td>415.90</td>\n",
              "      <td>413.60</td>\n",
              "      <td>414.95</td>\n",
              "      <td>64999</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>415.15</td>\n",
              "      <td>415.90</td>\n",
              "      <td>414.15</td>\n",
              "      <td>414.85</td>\n",
              "      <td>28525</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>414.10</td>\n",
              "      <td>418.80</td>\n",
              "      <td>413.55</td>\n",
              "      <td>415.15</td>\n",
              "      <td>152511</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>414.25</td>\n",
              "      <td>416.55</td>\n",
              "      <td>414.25</td>\n",
              "      <td>414.70</td>\n",
              "      <td>75048</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>414.25</td>\n",
              "      <td>415.05</td>\n",
              "      <td>413.30</td>\n",
              "      <td>414.25</td>\n",
              "      <td>15949</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>415.00</td>\n",
              "      <td>415.00</td>\n",
              "      <td>410.80</td>\n",
              "      <td>414.00</td>\n",
              "      <td>95718</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>413.55</td>\n",
              "      <td>416.70</td>\n",
              "      <td>412.60</td>\n",
              "      <td>414.50</td>\n",
              "      <td>46124</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>413.95</td>\n",
              "      <td>414.80</td>\n",
              "      <td>412.20</td>\n",
              "      <td>413.50</td>\n",
              "      <td>39154</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Close    High     Low    Open  Volume  class\n",
              "0   411.15  413.00  404.10  409.00  176240      1\n",
              "1   414.05  414.70  410.00  411.15   75844      1\n",
              "2   410.20  414.40  410.00  414.40   40789      1\n",
              "3   410.25  412.50  410.00  410.20   17987      1\n",
              "4   410.00  410.35  408.80  410.35   57179      1\n",
              "5   410.25  411.25  408.50  410.00   81484      1\n",
              "6   409.00  410.25  409.00  410.00   46254      1\n",
              "7   415.75  419.40  410.00  410.00  171503      0\n",
              "8   415.75  416.25  413.20  415.75  116767      0\n",
              "9   414.15  416.90  411.75  415.75   78892      0\n",
              "10  413.90  416.75  413.70  413.80   37042      1\n",
              "11  414.60  415.45  413.00  413.90   21333      0\n",
              "12  414.50  415.90  413.60  414.95   64999      1\n",
              "13  415.15  415.90  414.15  414.85   28525      1\n",
              "14  414.10  418.80  413.55  415.15  152511      1\n",
              "15  414.25  416.55  414.25  414.70   75048      1\n",
              "16  414.25  415.05  413.30  414.25   15949      0\n",
              "17  415.00  415.00  410.80  414.00   95718      0\n",
              "18  413.55  416.70  412.60  414.50   46124      0\n",
              "19  413.95  414.80  412.20  413.50   39154      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "hQMcIgziEQ-u",
        "colab_type": "code",
        "outputId": "0b517a69-cc84-4f7e-8f83-d83774613fa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "DjNxRFCjEQ-y",
        "colab_type": "code",
        "outputId": "7cb43e47-2949-429d-b5fb-52a0ca3f7308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y = df['class'].values\n",
        "y[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "BL_L5-xsEQ-1",
        "colab_type": "code",
        "outputId": "39dbaa95-3353-4261-8f7d-ad7f6ab79e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "X = df.drop('class', 1).values\n",
        "X[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   411.15,    413.  ,    404.1 ,    409.  , 176240.  ],\n",
              "       [   414.05,    414.7 ,    410.  ,    411.15,  75844.  ],\n",
              "       [   410.2 ,    414.4 ,    410.  ,    414.4 ,  40789.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "H2O1DBr3EQ-4",
        "colab_type": "code",
        "outputId": "b35be4a2-23e3-44f9-8fe9-24f10e058723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(X)\n",
        "x_scaled[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.18402965, 0.1838875 , 0.17836732, 0.18043088, 0.0146644 ],\n",
              "       [0.18749626, 0.18591348, 0.18546592, 0.18300419, 0.00597011],\n",
              "       [0.18289403, 0.18555595, 0.18546592, 0.18689408, 0.00293435]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "fpaMCALcEQ-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ConvModel(nn.Module):\n",
        "  def __init__(self, input_channel=5, kernel_size=10):\n",
        "    super(ConvModel, self).__init__()\n",
        "    self.c1 = nn.Conv1d(input_channel, 32, kernel_size)\n",
        "    self.c2 = nn.Conv1d(32, 64, kernel_size)\n",
        "    self.c3 = nn.Conv1d(64, 128, kernel_size)\n",
        "    self.c4 = nn.Conv1d(128, 256, kernel_size)\n",
        "    self.c5 = nn.Conv1d(256, 256, kernel_size)\n",
        "    self.c6 = nn.Conv1d(256, 256, kernel_size)\n",
        "    self.mp = nn.MaxPool1d(2)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.lrelu = nn.LeakyReLU(0.01)\n",
        "    self.dp = nn.Dropout(0.8)\n",
        "    self.fc1 = nn.Linear(3328, 2000)\n",
        "    self.fc2 = nn.Linear(2000, 1000)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.c1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.mp(x)\n",
        "    x = self.c2(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = self.mp(x)\n",
        "    \n",
        "    \n",
        "    x = self.c3(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = self.mp(x)\n",
        "    x = self.c4(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = self.mp(x)\n",
        "    x = self.c5(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = self.mp(x)\n",
        "    x = self.c6(x)\n",
        "    x = self.lrelu(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.dp(x)\n",
        "    x = self.fc2(x)\n",
        "#     x = self.sigmoid(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    return x\n",
        "    \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0e6TX82sEQ_B",
        "colab_type": "code",
        "outputId": "c976740a-6ab9-4e57-aae3-c970e7a2b823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2448
        }
      },
      "cell_type": "code",
      "source": [
        "model = ConvModel(5)\n",
        "\n",
        "import torch\n",
        "input1 = torch.randn(1, 5, 1000)\n",
        "model(input1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-6.8772, -6.8785, -6.8782, -6.9056, -6.9166, -6.9056, -6.8815,\n",
              "         -6.9153, -6.9097, -6.9340, -6.9342, -6.9256, -6.8809, -6.8585,\n",
              "         -6.9092, -6.9058, -6.9096, -6.9326, -6.9200, -6.9568, -6.9080,\n",
              "         -6.9066, -6.8840, -6.8989, -6.8950, -6.9111, -6.8817, -6.8984,\n",
              "         -6.8870, -6.9188, -6.8935, -6.8936, -6.8937, -6.9358, -6.9094,\n",
              "         -6.9418, -6.9161, -6.8832, -6.8708, -6.8898, -6.9276, -6.8690,\n",
              "         -6.9449, -6.9177, -6.8858, -6.9207, -6.9490, -6.8968, -6.8714,\n",
              "         -6.9144, -6.8904, -6.8971, -6.9158, -6.9047, -6.8909, -6.9225,\n",
              "         -6.9077, -6.9059, -6.8646, -6.9075, -6.9117, -6.9389, -6.9093,\n",
              "         -6.8965, -6.8860, -6.8854, -6.9000, -6.9076, -6.9308, -6.8818,\n",
              "         -6.9001, -6.8914, -6.9357, -6.9260, -6.8893, -6.8983, -6.9145,\n",
              "         -6.9307, -6.9063, -6.9436, -6.9054, -6.9431, -6.9094, -6.8612,\n",
              "         -6.9234, -6.9191, -6.8917, -6.9229, -6.8749, -6.9166, -6.8710,\n",
              "         -6.9108, -6.9010, -6.9325, -6.8865, -6.9194, -6.9228, -6.9072,\n",
              "         -6.9092, -6.9158, -6.9195, -6.9191, -6.9247, -6.9088, -6.8947,\n",
              "         -6.9042, -6.8985, -6.9073, -6.8633, -6.8868, -6.9368, -6.9058,\n",
              "         -6.9446, -6.8959, -6.8888, -6.9164, -6.9142, -6.8873, -6.8938,\n",
              "         -6.8962, -6.8995, -6.9321, -6.9183, -6.9305, -6.9317, -6.9060,\n",
              "         -6.9224, -6.9072, -6.8998, -6.9276, -6.9371, -6.8866, -6.9502,\n",
              "         -6.9334, -6.9211, -6.8621, -6.9488, -6.8787, -6.9261, -6.8915,\n",
              "         -6.9208, -6.8577, -6.9149, -6.9249, -6.8951, -6.9345, -6.9275,\n",
              "         -6.9721, -6.8802, -6.9271, -6.8983, -6.9218, -6.9091, -6.9164,\n",
              "         -6.9145, -6.9289, -6.9392, -6.9068, -6.8824, -6.8702, -6.8702,\n",
              "         -6.9110, -6.8781, -6.9202, -6.9223, -6.9309, -6.9024, -6.8962,\n",
              "         -6.9406, -6.8754, -6.9117, -6.9190, -6.8803, -6.9464, -6.9065,\n",
              "         -6.9420, -6.9257, -6.8665, -6.9104, -6.9010, -6.9213, -6.8894,\n",
              "         -6.8989, -6.9207, -6.9228, -6.9198, -6.9179, -6.8682, -6.9108,\n",
              "         -6.8953, -6.9064, -6.9015, -6.9164, -6.8842, -6.9114, -6.9095,\n",
              "         -6.9087, -6.8653, -6.8998, -6.9578, -6.9473, -6.9131, -6.9226,\n",
              "         -6.9330, -6.9239, -6.8891, -6.9133, -6.9075, -6.8857, -6.8860,\n",
              "         -6.9174, -6.9049, -6.9213, -6.9239, -6.9011, -6.9112, -6.9202,\n",
              "         -6.9090, -6.9414, -6.8974, -6.8750, -6.8655, -6.8658, -6.8752,\n",
              "         -6.8958, -6.8973, -6.9481, -6.9128, -6.8999, -6.9090, -6.9397,\n",
              "         -6.8933, -6.9187, -6.9305, -6.9262, -6.8994, -6.9307, -6.9147,\n",
              "         -6.8875, -6.8550, -6.9188, -6.9238, -6.9065, -6.9268, -6.9323,\n",
              "         -6.9160, -6.9195, -6.9132, -6.9239, -6.8857, -6.8907, -6.9088,\n",
              "         -6.9021, -6.9314, -6.9288, -6.8918, -6.8805, -6.9174, -6.9054,\n",
              "         -6.9107, -6.8921, -6.9032, -6.9342, -6.9001, -6.8861, -6.9109,\n",
              "         -6.8745, -6.9168, -6.8880, -6.9145, -6.8880, -6.9254, -6.9057,\n",
              "         -6.8985, -6.9408, -6.9080, -6.9209, -6.9182, -6.9210, -6.9406,\n",
              "         -6.9233, -6.9024, -6.9358, -6.8958, -6.9334, -6.8949, -6.9132,\n",
              "         -6.9497, -6.9140, -6.9166, -6.9329, -6.9169, -6.9309, -6.9159,\n",
              "         -6.9214, -6.9081, -6.9279, -6.9041, -6.8954, -6.9250, -6.8933,\n",
              "         -6.8998, -6.9276, -6.9241, -6.9003, -6.8590, -6.9274, -6.9129,\n",
              "         -6.9185, -6.9335, -6.8821, -6.8837, -6.8920, -6.8867, -6.8838,\n",
              "         -6.8670, -6.9058, -6.8593, -6.9017, -6.9093, -6.9183, -6.9066,\n",
              "         -6.9305, -6.9054, -6.9053, -6.8800, -6.8781, -6.8812, -6.9292,\n",
              "         -6.8882, -6.9397, -6.8634, -6.9317, -6.8816, -6.8905, -6.9074,\n",
              "         -6.9110, -6.9475, -6.8924, -6.8632, -6.9251, -6.8833, -6.9233,\n",
              "         -6.9302, -6.8981, -6.9101, -6.8709, -6.9104, -6.9051, -6.9232,\n",
              "         -6.9346, -6.9182, -6.9034, -6.8829, -6.8965, -6.9094, -6.9039,\n",
              "         -6.9003, -6.9168, -6.8800, -6.9342, -6.9283, -6.9171, -6.8887,\n",
              "         -6.9100, -6.9171, -6.9380, -6.8969, -6.8674, -6.9111, -6.9093,\n",
              "         -6.8479, -6.8936, -6.9284, -6.9146, -6.9352, -6.8961, -6.9623,\n",
              "         -6.8645, -6.8703, -6.8670, -6.8959, -6.8583, -6.9327, -6.9145,\n",
              "         -6.9130, -6.8707, -6.9241, -6.9046, -6.9005, -6.9021, -6.8828,\n",
              "         -6.9184, -6.9134, -6.8834, -6.9019, -6.9040, -6.9173, -6.9281,\n",
              "         -6.8934, -6.9051, -6.9254, -6.8855, -6.8812, -6.9341, -6.8922,\n",
              "         -6.9142, -6.8741, -6.8587, -6.8698, -6.8974, -6.8888, -6.9502,\n",
              "         -6.8697, -6.9209, -6.9241, -6.9295, -6.9071, -6.9161, -6.9016,\n",
              "         -6.9058, -6.9396, -6.9133, -6.9021, -6.9110, -6.9155, -6.8851,\n",
              "         -6.9163, -6.9259, -6.8972, -6.9220, -6.8736, -6.9059, -6.8794,\n",
              "         -6.9219, -6.9295, -6.8987, -6.9342, -6.9184, -6.9321, -6.9276,\n",
              "         -6.8948, -6.9138, -6.9501, -6.9052, -6.9507, -6.8768, -6.9149,\n",
              "         -6.9445, -6.9083, -6.9170, -6.8830, -6.9093, -6.9209, -6.8890,\n",
              "         -6.9253, -6.8782, -6.9279, -6.8884, -6.8895, -6.9019, -6.9057,\n",
              "         -6.8926, -6.9147, -6.9072, -6.8739, -6.9191, -6.9275, -6.9107,\n",
              "         -6.9486, -6.9365, -6.9254, -6.9210, -6.8915, -6.9095, -6.9274,\n",
              "         -6.9504, -6.9126, -6.8585, -6.9498, -6.8795, -6.8960, -6.9382,\n",
              "         -6.8834, -6.9148, -6.9063, -6.9074, -6.9004, -6.9060, -6.8887,\n",
              "         -6.9059, -6.9301, -6.8972, -6.9055, -6.9129, -6.8801, -6.9133,\n",
              "         -6.8727, -6.9087, -6.9077, -6.8884, -6.8896, -6.9277, -6.8953,\n",
              "         -6.9140, -6.9493, -6.9165, -6.9189, -6.9411, -6.8696, -6.9037,\n",
              "         -6.9347, -6.9098, -6.9164, -6.8820, -6.9333, -6.9163, -6.9202,\n",
              "         -6.9250, -6.9013, -6.9147, -6.8865, -6.9132, -6.8612, -6.8850,\n",
              "         -6.9413, -6.9150, -6.9328, -6.9133, -6.9116, -6.9038, -6.8951,\n",
              "         -6.9152, -6.9028, -6.8946, -6.9085, -6.9109, -6.9371, -6.9473,\n",
              "         -6.9381, -6.9122, -6.8901, -6.9450, -6.8768, -6.9031, -6.9114,\n",
              "         -6.8644, -6.9505, -6.9042, -6.9262, -6.8899, -6.9206, -6.8847,\n",
              "         -6.9321, -6.9034, -6.9298, -6.9011, -6.8877, -6.8904, -6.9017,\n",
              "         -6.9353, -6.9010, -6.9351, -6.9326, -6.9261, -6.9177, -6.9440,\n",
              "         -6.8879, -6.8908, -6.9500, -6.8798, -6.8895, -6.8978, -6.8857,\n",
              "         -6.8679, -6.9033, -6.8841, -6.9239, -6.9121, -6.9499, -6.9210,\n",
              "         -6.9103, -6.8986, -6.9008, -6.8873, -6.9377, -6.9255, -6.9082,\n",
              "         -6.9139, -6.8699, -6.9037, -6.9065, -6.9177, -6.9037, -6.9357,\n",
              "         -6.9012, -6.9345, -6.8985, -6.8779, -6.8910, -6.8911, -6.8906,\n",
              "         -6.9132, -6.9171, -6.9211, -6.9420, -6.9436, -6.8883, -6.9276,\n",
              "         -6.8924, -6.9290, -6.8992, -6.8911, -6.8947, -6.9180, -6.9078,\n",
              "         -6.8929, -6.9372, -6.9667, -6.9242, -6.9113, -6.9092, -6.8882,\n",
              "         -6.8933, -6.9025, -6.9293, -6.9363, -6.9308, -6.8918, -6.9133,\n",
              "         -6.9086, -6.8898, -6.9288, -6.9228, -6.8753, -6.9067, -6.8837,\n",
              "         -6.9081, -6.9203, -6.9255, -6.9216, -6.8820, -6.9032, -6.8788,\n",
              "         -6.9062, -6.9199, -6.9095, -6.9271, -6.9296, -6.8945, -6.9029,\n",
              "         -6.8866, -6.9177, -6.8849, -6.8893, -6.9330, -6.8757, -6.8582,\n",
              "         -6.8834, -6.8976, -6.9294, -6.8675, -6.9417, -6.9175, -6.9165,\n",
              "         -6.9202, -6.8786, -6.9012, -6.8979, -6.9072, -6.8992, -6.9024,\n",
              "         -6.9096, -6.9410, -6.8848, -6.9181, -6.9186, -6.9291, -6.9078,\n",
              "         -6.9358, -6.8898, -6.9147, -6.8769, -6.8717, -6.8681, -6.8784,\n",
              "         -6.8852, -6.8860, -6.9054, -6.9157, -6.8837, -6.8875, -6.9044,\n",
              "         -6.9128, -6.8852, -6.9283, -6.8946, -6.9181, -6.9248, -6.9375,\n",
              "         -6.9172, -6.8837, -6.9038, -6.8996, -6.8809, -6.8732, -6.8871,\n",
              "         -6.9102, -6.9052, -6.8969, -6.9110, -6.9320, -6.9034, -6.8925,\n",
              "         -6.9381, -6.9010, -6.8885, -6.9159, -6.9052, -6.9271, -6.9145,\n",
              "         -6.9070, -6.9410, -6.9176, -6.9115, -6.9414, -6.9211, -6.9307,\n",
              "         -6.9135, -6.9492, -6.9395, -6.8979, -6.9245, -6.8859, -6.9307,\n",
              "         -6.9026, -6.9198, -6.9056, -6.9132, -6.8956, -6.9000, -6.8762,\n",
              "         -6.8987, -6.8943, -6.9076, -6.9205, -6.8993, -6.8814, -6.9043,\n",
              "         -6.9091, -6.9098, -6.8979, -6.9175, -6.8644, -6.9174, -6.9489,\n",
              "         -6.9000, -6.9048, -6.8635, -6.9126, -6.8926, -6.9255, -6.9059,\n",
              "         -6.9157, -6.9071, -6.9314, -6.9177, -6.9474, -6.8960, -6.8866,\n",
              "         -6.9047, -6.8963, -6.9203, -6.9332, -6.9088, -6.9027, -6.9095,\n",
              "         -6.9065, -6.9142, -6.9471, -6.9182, -6.8548, -6.8806, -6.9571,\n",
              "         -6.9176, -6.9476, -6.9201, -6.8878, -6.9022, -6.9273, -6.9346,\n",
              "         -6.9110, -6.9403, -6.9253, -6.8405, -6.9205, -6.9255, -6.9026,\n",
              "         -6.8893, -6.8841, -6.9000, -6.9179, -6.8882, -6.8947, -6.9173,\n",
              "         -6.9278, -6.9001, -6.9030, -6.9208, -6.8967, -6.9033, -6.9030,\n",
              "         -6.8583, -6.9048, -6.9198, -6.8966, -6.9215, -6.8854, -6.9231,\n",
              "         -6.9180, -6.9009, -6.8993, -6.9550, -6.9012, -6.9299, -6.8639,\n",
              "         -6.9013, -6.8847, -6.8721, -6.9194, -6.9267, -6.8941, -6.9096,\n",
              "         -6.8905, -6.9060, -6.9103, -6.9082, -6.9141, -6.8975, -6.8838,\n",
              "         -6.9007, -6.8968, -6.9029, -6.9041, -6.8923, -6.8816, -6.9057,\n",
              "         -6.8914, -6.9013, -6.9189, -6.9188, -6.9302, -6.9234, -6.9172,\n",
              "         -6.8915, -6.9197, -6.9120, -6.9327, -6.9039, -6.8690, -6.9212,\n",
              "         -6.9256, -6.9070, -6.9035, -6.9298, -6.8928, -6.8644, -6.9352,\n",
              "         -6.9308, -6.9014, -6.9477, -6.9249, -6.8947, -6.9229, -6.9442,\n",
              "         -6.9439, -6.9031, -6.8711, -6.8998, -6.9281, -6.8970, -6.9337,\n",
              "         -6.9127, -6.9054, -6.9275, -6.9262, -6.9214, -6.8889, -6.9395,\n",
              "         -6.9225, -6.9052, -6.8928, -6.9229, -6.8857, -6.9200, -6.8710,\n",
              "         -6.8907, -6.9089, -6.9345, -6.9012, -6.9247, -6.8927, -6.9199,\n",
              "         -6.8876, -6.9222, -6.9024, -6.9061, -6.8841, -6.8836, -6.9140,\n",
              "         -6.9369, -6.9346, -6.9493, -6.9277, -6.9376, -6.9343, -6.9177,\n",
              "         -6.9108, -6.8968, -6.9454, -6.9081, -6.9115, -6.9290, -6.9299,\n",
              "         -6.8802, -6.9071, -6.9135, -6.8751, -6.9292, -6.8853, -6.9158,\n",
              "         -6.8774, -6.8960, -6.9187, -6.9174, -6.9093, -6.8748, -6.8936,\n",
              "         -6.9239, -6.8810, -6.9096, -6.9004, -6.8682, -6.8967, -6.9259,\n",
              "         -6.9276, -6.9105, -6.9559, -6.9425, -6.9241, -6.9315, -6.8660,\n",
              "         -6.8716, -6.9115, -6.9077, -6.9196, -6.8977, -6.9052, -6.8866,\n",
              "         -6.9104, -6.9143, -6.8820, -6.9014, -6.9204, -6.9076, -6.9355,\n",
              "         -6.8904, -6.9153, -6.9013, -6.8965, -6.8722, -6.9280, -6.9254,\n",
              "         -6.8512, -6.9203, -6.9158, -6.9259, -6.9054, -6.8714, -6.8931,\n",
              "         -6.9127, -6.9429, -6.9529, -6.9348, -6.9163, -6.8902, -6.8922,\n",
              "         -6.8911, -6.9316, -6.9270, -6.9166, -6.9287, -6.8688, -6.9030,\n",
              "         -6.9229, -6.9223, -6.8961, -6.8881, -6.9149, -6.9068]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "dNRgUjVfEQ_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6e5QAbKFEQ_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X_n = []\n",
        "y_n = []\n",
        "\n",
        "for i in range(len(x_scaled) - 1000):\n",
        "    X_n.append(x_scaled[i:i+1000].T)\n",
        "    y_n.append(np.array(y[i:i+1000]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WxjihxnYdqUg",
        "colab_type": "code",
        "outputId": "b3034f16-1d4a-400e-c7f2-cf673486a72b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_n[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "O9sM93EwEQ_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_n[:int(len(X_n)*0.8)]\n",
        "X_test = X_n[len(X_n[:int(len(X_n)*0.8)]):]\n",
        "\n",
        "y_train = y_n[:int(len(y_n)*0.8)]\n",
        "y_test = y_n[len(y_n[:int(len(y_n)*0.8)]):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fUIa4yHHa-LL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_n, y_n, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "39LvL_aBa-dL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xmVWND1IEQ_V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.utils.data as utils\n",
        "\n",
        "tensor_x_tr = torch.stack([torch.Tensor(i) for i in X_n]) \n",
        "tensor_y_tr = torch.stack([torch.Tensor(i) for i in y_n])\n",
        "\n",
        "my_dataset_tr = utils.TensorDataset(tensor_x_tr,tensor_y_tr) \n",
        "trainloader = utils.DataLoader(my_dataset_tr, batch_size=1024) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tensor_x_te = torch.stack([torch.Tensor(i) for i in X_test]) \n",
        "tensor_y_te = torch.stack([torch.Tensor(i) for i in y_test])\n",
        "\n",
        "my_dataset_te = utils.TensorDataset(tensor_x_te,tensor_y_te) \n",
        "testloader = utils.DataLoader(my_dataset_te, batch_size=1024) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m-p4tIR8EQ_a",
        "colab_type": "code",
        "outputId": "aa67f79f-8154-4edc-db5e-324306814eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "for data, target in trainloader:\n",
        "    print(data.shape)\n",
        "    print(target.shape)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1024, 5, 1000])\n",
            "torch.Size([1024, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tU-qhWW4EQ_e",
        "colab_type": "code",
        "outputId": "d2f03020-59ee-4c12-f5e3-fb265c5b1ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "model =  ConvModel(5)\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvModel(\n",
            "  (c1): Conv1d(5, 32, kernel_size=(10,), stride=(1,))\n",
            "  (c2): Conv1d(32, 64, kernel_size=(10,), stride=(1,))\n",
            "  (c3): Conv1d(64, 128, kernel_size=(10,), stride=(1,))\n",
            "  (c4): Conv1d(128, 256, kernel_size=(10,), stride=(1,))\n",
            "  (c5): Conv1d(256, 256, kernel_size=(10,), stride=(1,))\n",
            "  (c6): Conv1d(256, 256, kernel_size=(10,), stride=(1,))\n",
            "  (mp): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (relu): ReLU()\n",
            "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
            "  (dp): Dropout(p=0.8)\n",
            "  (fc1): Linear(in_features=3328, out_features=2000, bias=True)\n",
            "  (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZfT2c8EdEQ_h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model =  ConvModel(5)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PLMtyDbcv43",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cwrDUN4Y5RP5",
        "colab_type": "code",
        "outputId": "a259d84a-23c5-43fb-fce8-d86269d5683b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "epochs = 5\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 5\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in trainloader:\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "        labels = labels.long()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logps = model(inputs)\n",
        "        loss = criterion(logps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "#         if steps % print_every == 0:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "#                     inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "            labels=labels.long()\n",
        "            logps = model.forward(inputs)\n",
        "            batch_loss = criterion(logps, labels)\n",
        "\n",
        "            test_loss += batch_loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            ps = torch.exp(logps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "#                     print('acc=',accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "          f\"Train loss: {running_loss/print_every}.. \"\n",
        "          f\"Test loss: {test_loss/len(testloader)}.. \"\n",
        "          f\"Test accuracy: {accuracy/len(testloader)}\")\n",
        "    running_loss = 0\n",
        "    model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5.. Train loss: 8.277824211120606.. Test loss: 6.879637718200684.. Test accuracy: 0.0\n",
            "Epoch 2/5.. Train loss: 8.248946952819825.. Test loss: 6.8454749584198.. Test accuracy: 0.73876953125\n",
            "Epoch 3/5.. Train loss: 8.20350341796875.. Test loss: 6.801614284515381.. Test accuracy: 0.73876953125\n",
            "Epoch 4/5.. Train loss: 8.149008750915527.. Test loss: 6.752331733703613.. Test accuracy: 0.73876953125\n",
            "Epoch 5/5.. Train loss: 8.089841842651367.. Test loss: 6.699279546737671.. Test accuracy: 0.73876953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o1P27inu_LRa",
        "colab_type": "code",
        "outputId": "f2d42580-98c7-4e29-ee12-ad0b2f17639c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "epochs = 15\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 5\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in trainloader:\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "        labels = labels.long()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logps = model(inputs)\n",
        "        loss = criterion(logps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "#         if steps % print_every == 0:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "#                     inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "            labels=labels.long()\n",
        "            logps = model.forward(inputs)\n",
        "            batch_loss = criterion(logps, labels)\n",
        "\n",
        "            test_loss += batch_loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            ps = torch.exp(logps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "#                     print('acc=',accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "          f\"Train loss: {running_loss/print_every}.. \"\n",
        "          f\"Test loss: {test_loss/len(testloader)}.. \"\n",
        "          f\"Test accuracy: {accuracy/len(testloader)}\")\n",
        "    running_loss = 0\n",
        "    model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15.. Train loss: 8.047703742980957.. Test loss: 6.682156085968018.. Test accuracy: 0.73876953125\n",
            "Epoch 2/15.. Train loss: 8.015548419952392.. Test loss: 6.644542455673218.. Test accuracy: 0.73876953125\n",
            "Epoch 3/15.. Train loss: 7.965180778503418.. Test loss: 6.59398889541626.. Test accuracy: 0.73876953125\n",
            "Epoch 4/15.. Train loss: 7.9010422706604.. Test loss: 6.533164978027344.. Test accuracy: 0.73876953125\n",
            "Epoch 5/15.. Train loss: 7.825938987731933.. Test loss: 6.461606979370117.. Test accuracy: 0.73876953125\n",
            "Epoch 6/15.. Train loss: 7.738792133331299.. Test loss: 6.375495433807373.. Test accuracy: 0.73876953125\n",
            "Epoch 7/15.. Train loss: 7.632960510253906.. Test loss: 6.26566481590271.. Test accuracy: 0.73876953125\n",
            "Epoch 8/15.. Train loss: 7.494869804382324.. Test loss: 6.108920097351074.. Test accuracy: 0.73876953125\n",
            "Epoch 9/15.. Train loss: 7.289493370056152.. Test loss: 5.834909915924072.. Test accuracy: 0.73876953125\n",
            "Epoch 10/15.. Train loss: 6.884248161315918.. Test loss: 5.122793197631836.. Test accuracy: 0.73876953125\n",
            "Epoch 11/15.. Train loss: 5.4160651683807375.. Test loss: 1.1668009459972382.. Test accuracy: 0.73876953125\n",
            "Epoch 12/15.. Train loss: 2.4627681136131288.. Test loss: 0.734435647726059.. Test accuracy: 0.26123046875\n",
            "Epoch 13/15.. Train loss: 2.1454816818237306.. Test loss: 3.5658298808546647.. Test accuracy: 0.73876953125\n",
            "Epoch 14/15.. Train loss: 4.316073346138.. Test loss: 1.9718993306159973.. Test accuracy: 0.26123046875\n",
            "Epoch 15/15.. Train loss: 2.466879105567932.. Test loss: 3.223760962486267.. Test accuracy: 0.26123046875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AkIVYuwgEQ_t",
        "colab_type": "code",
        "outputId": "ae8a2a66-b3a0-49e1-9090-8f658c103019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "epochs = 15\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 5\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in trainloader:\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "        labels = labels.long()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logps = model(inputs)\n",
        "        loss = criterion(logps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "#         if steps % print_every == 0:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "#                     inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "            labels=labels.long()\n",
        "            logps = model.forward(inputs)\n",
        "            batch_loss = criterion(logps, labels)\n",
        "\n",
        "            test_loss += batch_loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            ps = torch.exp(logps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "#                     print('acc=',accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "          f\"Train loss: {running_loss/print_every}.. \"\n",
        "          f\"Test loss: {test_loss/len(testloader)}.. \"\n",
        "          f\"Test accuracy: {accuracy/len(testloader)}\")\n",
        "    running_loss = 0\n",
        "    model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15.. Train loss: 3.1828015804290772.. Test loss: 3.186503291130066.. Test accuracy: 0.26123046875\n",
            "Epoch 2/15.. Train loss: 3.0239305019378664.. Test loss: 3.1701241731643677.. Test accuracy: 0.26123046875\n",
            "Epoch 3/15.. Train loss: 2.866197347640991.. Test loss: 3.172333598136902.. Test accuracy: 0.26123046875\n",
            "Epoch 4/15.. Train loss: 2.7391216278076174.. Test loss: 3.099419951438904.. Test accuracy: 0.26123046875\n",
            "Epoch 5/15.. Train loss: 2.5808584213256838.. Test loss: 2.9255017042160034.. Test accuracy: 0.26123046875\n",
            "Epoch 6/15.. Train loss: 2.421225643157959.. Test loss: 2.692746937274933.. Test accuracy: 0.26123046875\n",
            "Epoch 7/15.. Train loss: 2.2383219957351685.. Test loss: 2.448759377002716.. Test accuracy: 0.26123046875\n",
            "Epoch 8/15.. Train loss: 2.084571385383606.. Test loss: 2.215104103088379.. Test accuracy: 0.26123046875\n",
            "Epoch 9/15.. Train loss: 1.917207670211792.. Test loss: 1.9873838424682617.. Test accuracy: 0.26123046875\n",
            "Epoch 10/15.. Train loss: 1.7300297975540162.. Test loss: 1.7635365724563599.. Test accuracy: 0.26123046875\n",
            "Epoch 11/15.. Train loss: 1.547270941734314.. Test loss: 1.544143557548523.. Test accuracy: 0.26123046875\n",
            "Epoch 12/15.. Train loss: 1.3777713179588318.. Test loss: 1.3299208879470825.. Test accuracy: 0.26123046875\n",
            "Epoch 13/15.. Train loss: 1.2312032341957093.. Test loss: 1.1293288469314575.. Test accuracy: 0.26123046875\n",
            "Epoch 14/15.. Train loss: 1.0963495373725891.. Test loss: 0.956928014755249.. Test accuracy: 0.26123046875\n",
            "Epoch 15/15.. Train loss: 0.9952785611152649.. Test loss: 0.8255801796913147.. Test accuracy: 0.26123046875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BWeynJBFLwrS",
        "colab_type": "code",
        "outputId": "0444284f-4268-477f-e6cd-1575bec769b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "epochs = 15\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 5\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in trainloader:\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "        labels = labels.long()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logps = model(inputs)\n",
        "        loss = criterion(logps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "#         if steps % print_every == 0:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "#                     inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "            labels=labels.long()\n",
        "            logps = model.forward(inputs)\n",
        "            batch_loss = criterion(logps, labels)\n",
        "\n",
        "            test_loss += batch_loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            ps = torch.exp(logps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "#                     print('acc=',accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "          f\"Train loss: {running_loss/print_every}.. \"\n",
        "          f\"Test loss: {test_loss/len(testloader)}.. \"\n",
        "          f\"Test accuracy: {accuracy/len(testloader)}\")\n",
        "    running_loss = 0\n",
        "    model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15.. Train loss: 0.960979413986206.. Test loss: 0.8127917349338531.. Test accuracy: 0.26123046875\n",
            "Epoch 2/15.. Train loss: 0.940903389453888.. Test loss: 0.7788000404834747.. Test accuracy: 0.26123046875\n",
            "Epoch 3/15.. Train loss: 0.9320174217224121.. Test loss: 0.7417958974838257.. Test accuracy: 0.26123046875\n",
            "Epoch 4/15.. Train loss: 0.8921349048614502.. Test loss: 0.710993081331253.. Test accuracy: 0.26123046875\n",
            "Epoch 5/15.. Train loss: 0.8851672410964966.. Test loss: 0.6896490454673767.. Test accuracy: 0.73876953125\n",
            "Epoch 6/15.. Train loss: 0.8996837973594666.. Test loss: 0.6781420409679413.. Test accuracy: 0.73876953125\n",
            "Epoch 7/15.. Train loss: 0.8852800846099853.. Test loss: 0.6732870936393738.. Test accuracy: 0.73876953125\n",
            "Epoch 8/15.. Train loss: 0.8686652064323426.. Test loss: 0.6729720830917358.. Test accuracy: 0.73876953125\n",
            "Epoch 9/15.. Train loss: 0.8810382843017578.. Test loss: 0.6764772236347198.. Test accuracy: 0.73876953125\n",
            "Epoch 10/15.. Train loss: 0.883145010471344.. Test loss: 0.6803750693798065.. Test accuracy: 0.73876953125\n",
            "Epoch 11/15.. Train loss: 0.8691751003265381.. Test loss: 0.6832975447177887.. Test accuracy: 0.73876953125\n",
            "Epoch 12/15.. Train loss: 0.8759678483009339.. Test loss: 0.6849410831928253.. Test accuracy: 0.73876953125\n",
            "Epoch 13/15.. Train loss: 0.8752351522445678.. Test loss: 0.6842301487922668.. Test accuracy: 0.73876953125\n",
            "Epoch 14/15.. Train loss: 0.8695126652717591.. Test loss: 0.6837323904037476.. Test accuracy: 0.73876953125\n",
            "Epoch 15/15.. Train loss: 0.8764691233634949.. Test loss: 0.6845024526119232.. Test accuracy: 0.73876953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NvfUS1O1UJK5",
        "colab_type": "code",
        "outputId": "11c55a40-e7ea-45c1-82ea-fd4ed272de53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "epochs = 15\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 1\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in trainloader:\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "#         inputs, labels = inputs.to(device), labels.to(device)\n",
        "        labels = labels.long()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        print(f'labels s={labels.view(-1).shape}, data s={data.shape}')\n",
        "        logps = model(inputs)\n",
        "        print(f'logps shape{logps.view(-1).shape}')\n",
        "        loss = criterion(logps.view(1,-1), labels.view(1,-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "#         if steps % print_every == 0:\n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "    #                     inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "                labels=labels.long()\n",
        "#                 print(f'labels shape = {labels.shape} ')\n",
        "                print(f'labels s={labels.shape}, data s={data.shape}')\n",
        "                logps = model.forward(inputs)\n",
        "                batch_loss = criterion(logps, labels)\n",
        "\n",
        "                test_loss += batch_loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                ps = torch.exp(logps)\n",
        "                print(f'epoch={epoch} ps shape = {ps.shape}')\n",
        "                print(f'labels shape = {labels.shape} ')\n",
        "#                 top_p, top_class = ps.topk(1, dim=1)\n",
        "#                 print('topp shape= ',top_p.shape)\n",
        "                equals = ps == labels\n",
        "#   .view(*ps.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "    #                     print('acc=',accuracy)\n",
        "\n",
        "#         print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "#               f\"Train loss: {running_loss/print_every}.. \"\n",
        "#               f\"Test loss: {test_loss/len(testloader)}.. \"\n",
        "#               f\"Test accuracy: {accuracy/len(testloader)}\")\n",
        "        running_loss = 0\n",
        "        model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels s=torch.Size([1024000]), data s=torch.Size([1024, 5, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OGdRrj2_aaHe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}